# ğŸ“˜ SFTTrainer Quick Reference

**SFTTrainer** (Supervised Fine-Tuning Trainer) lÃ  lá»›p chÃ­nh trong thÆ° viá»‡n `trl` dÃ¹ng Ä‘á»ƒ tinh chá»‰nh LLM trÃªn táº­p dá»¯ liá»‡u cÃ³ hÆ°á»›ng dáº«n (instruction datasets).

## ğŸ› ï¸ Cáº¥u hÃ¬nh quan trá»ng (`SFTConfig`)

Sá»­ dá»¥ng `SFTConfig` (káº¿ thá»«a `TrainingArguments`) Ä‘á»ƒ kiá»ƒm soÃ¡t quÃ¡ trÃ¬nh huáº¥n luyá»‡n.

### 1\. Batch Size & Gradient


| Tham sá»‘ | Ã nghÄ©a & Äá»‹nh nghÄ©a |
| :--- | :--- |
| `per_device_train_batch_size` | **Micro-Batch Size:** Sá»‘ lÆ°á»£ng máº«u dá»¯ liá»‡u thá»±c táº¿ Ä‘Æ°á»£c náº¡p vÃ o VRAM cá»§a **1 GPU** táº¡i má»™t thá»i Ä‘iá»ƒm. <br> *Quyáº¿t Ä‘á»‹nh viá»‡c cÃ³ bá»‹ OOM (trÃ n bá»™ nhá»›) hay khÃ´ng.* |
| `gradient_accumulation_steps` | **Accumulation:** Sá»‘ bÆ°á»›c "chá»" Ä‘á»ƒ tÃ­ch lÅ©y gradient trÆ°á»›c khi thá»±c sá»± cáº­p nháº­t trá»ng sá»‘ (weight update). GiÃºp mÃ´ phá»ng batch lá»›n trÃªn GPU yáº¿u. |
| **Effective / Total Batch Size** | **KÃ­ch thÆ°á»›c Batch thá»±c táº¿:** Sá»‘ lÆ°á»£ng máº«u dá»¯ liá»‡u mÃ´ hÃ¬nh "nhÃ¬n tháº¥y" trÆ°á»›c khi thá»±c hiá»‡n **1 bÆ°á»›c cáº­p nháº­t trá»ng sá»‘ (1 step)**. <br> **CÃ´ng thá»©c:** `Per_Device * Accumulation * Sá»‘ lÆ°á»£ng GPUs`. |

### 2\. ğŸ§® CÃ¡ch tÃ­nh toÃ¡n Steps & Epochs (Training Dynamics)

Hiá»ƒu cÃ¡ch tÃ­nh nÃ y giÃºp báº¡n Æ°á»›c lÆ°á»£ng thá»i gian train vÃ  chá»n `max_steps` phÃ¹ há»£p.

**CÃ´ng thá»©c cá»‘t lÃµi:**
$$Steps\_Per\_Epoch = \frac{Total\_Dataset\_Size}{Effective\_Batch\_Size}$$

**VÃ­ dá»¥ minh há»a:**
Giáº£ sá»­ báº¡n cÃ³ cáº¥u hÃ¬nh sau:

  * **Dataset:** 10,000 máº«u (samples).
  * 2 GPUs.
  * **Config:** `per_device_train_batch_size=4`, `gradient_accumulation_steps=8`.
  * **Má»¥c tiÃªu:** Train trong 3 Epochs.

**TÃ­nh toÃ¡n:**

1.  **TÃ­nh Effective Batch Size:**
    $$4 \text{ (máº«u/GPU)} \times 8 \text{ (accumulation)} \times 2 \text{ (GPUs)} = \textbf{64 máº«u/step}$$
    *(NghÄ©a lÃ  má»—i láº§n model cáº­p nháº­t trá»ng sá»‘, nÃ³ Ä‘Ã£ há»c tá»« 64 máº«u dá»¯ liá»‡u).*

2.  **TÃ­nh sá»‘ bÆ°á»›c trong 1 Epoch:**
    $$10,000 / 64 = 156.25 \rightarrow \textbf{157 steps} \text{ (lÃ m trÃ²n lÃªn)}$$

3.  **Tá»•ng sá»‘ bÆ°á»›c training (Total Steps):**
    $$157 \text{ steps} \times 3 \text{ epochs} = \textbf{471 steps}$$

> [\!TIP]
> **NghÄ©a lÃ **
> Náº¿u set `max_steps=1000` cho dataset trÃªn, mÃ´ hÃ¬nh sáº½ train khoáº£ng **6.4 Epochs** ($1000 / 157$). 

### 3\. TÃ i nguyÃªn & Tá»‘c Ä‘á»™

| Tham sá»‘ | Ã nghÄ©a & KhuyÃªn dÃ¹ng |
| :--- | :--- |
| `gradient_checkpointing` | `True`. Hy sinh tá»‘c Ä‘á»™ tÃ­nh toÃ¡n (cháº­m hÆ¡n \~20%) Ä‘á»ƒ giáº£m máº¡nh VRAM (lÆ°u Ã­t activation hÆ¡n). Báº¯t buá»™c vá»›i model lá»›n. |
| `bf16` | `True` (náº¿u GPU há»— trá»£ Ampere trá»Ÿ lÃªn). TÄƒng tá»‘c vÃ  giáº£m bá»™ nhá»› so vá»›i FP32, á»•n Ä‘á»‹nh hÆ¡n FP16. |

### 4\. Chiáº¿n lÆ°á»£c Train (Steps vs Epochs)

| Tham sá»‘ | Ã nghÄ©a |
| :--- | :--- |
| `num_train_epochs` | Sá»‘ láº§n model duyá»‡t qua toÃ n bá»™ dataset. |
| `max_steps` | Sá»‘ bÆ°á»›c update weights tuyá»‡t Ä‘á»‘i (sáº½ ghi Ä‘Ã¨ `num_train_epochs`). |

> [\!NOTE]
>
>   * **Dataset nhá» (\< 10k):** Set `num_train_epochs = 3`.
>   * **Dataset lá»›n (\> 50k):** Set `max_steps`. ThÆ°á»ng chá»‰ cáº§n **1000 - 2000 steps** (báº¥t ká»ƒ bao nhiÃªu epoch) lÃ  model Ä‘Ã£ há»c tá»‘t hÆ°á»›ng dáº«n (alignment). Train full epoch vá»›i dataset khá»•ng lá»“ thÆ°á»ng lÃ£ng phÃ­ vÃ  gÃ¢y quÃªn kiáº¿n thá»©c (catastrophic forgetting).

### 5\. Learning Rate

| Tham sá»‘ | Ã nghÄ©a & KhuyÃªn dÃ¹ng |
| :--- | :--- |
| `learning_rate` | Tá»‘c Ä‘á»™ há»c. <br> **QLoRA/LoRA:** `2e-4`. <br> **Full Fine-tune:** `1e-5` Ä‘áº¿n `2e-5`. |
| `lr_scheduler_type` | `"cosine"` (mÆ°á»£t mÃ  giáº£m dáº§n) hoáº·c `"constant_with_warmup"`. |
| `warmup_ratio` | `0.03` (3% tá»•ng steps). GiÃºp model lÃ m quen dá»¯ liá»‡u tá»« tá»«, trÃ¡nh shock gradient Ä‘áº§u chu ká»³. |



## ğŸ“ Äá»‹nh dáº¡ng dá»¯ liá»‡u (Dataset Format)

**1. Conversational (Chuáº©n Chat - KhuyÃªn dÃ¹ng):**
Tá»± Ä‘á»™ng Ã¡p dá»¥ng chat template.

```json
{"messages": [{"role": "user", "content": "Hello"}, {"role": "assistant", "content": "Hi there!"}]}
```

**2. Instruction/Response (Cá»• Ä‘iá»ƒn):**

```json
{"prompt": "User: Hello\nAssistant:", "completion": " Hi there!"}
```



## ğŸš€ Code máº«u (Minimal Snippet)

Äoáº¡n code dÆ°á»›i Ä‘Ã¢y setup Ä‘á»ƒ Ä‘áº¡t **Effective Batch Size = 16** trÃªn 1 GPU.

```python
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset

# Giáº£ sá»­ dataset cÃ³ 1000 máº«u
dataset = load_dataset("trl-lib/Capybara", split="train").select(range(1000))

args = SFTConfig(
    output_dir="./qwen_finetuned",
    max_seq_length=2048,
    packing=True,                   # Gom data Ä‘á»ƒ train nhanh hÆ¡n
    # --- Cáº¥u hÃ¬nh Batch Size ---
    per_device_train_batch_size=2,  # Má»—i láº§n GPU load 2 máº«u (Ä‘á»ƒ khÃ´ng OOM)
    gradient_accumulation_steps=8,  # TÃ­ch lÅ©y 8 láº§n má»›i update
    # => Effective Batch Size = 2 * 8 * 1(GPU) = 16
    # --- Cáº¥u hÃ¬nh Steps ---
    num_train_epochs=3,             # Tá»•ng steps sáº½ lÃ : (1000 / 16) * 3 ~= 189 steps
    
    learning_rate=2e-4,             # LR cho LoRA
    logging_steps=10,
    bf16=True,
    report_to="none"
)

trainer = SFTTrainer(
    model="Qwen/Qwen2.5-1.5B-Instruct",
    train_dataset=dataset,
    args=args,
)

trainer.train()
```